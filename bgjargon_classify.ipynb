{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "from distance import levenshteinDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting initial list of bad words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English bad words translated to bulgarian. <br>\n",
    "And from bulgarian to english. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/english-bad-words-translated.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/english-bad-words.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      2\u001b[0m     english_original \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mrstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file]\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/english-bad-words-translated.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m     english_translated \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mrstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/english-bad-words-translated-back.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[1;32mc:\\Users\\nikip\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/english-bad-words-translated.txt'"
     ]
    }
   ],
   "source": [
    "with open(\"data/english-bad-words.txt\") as file:\n",
    "    english_original = [line.rstrip() for line in file]\n",
    "\n",
    "with open(\"data/english-bad-words-translated.txt\") as file:\n",
    "    english_translated = [line.rstrip() for line in file]\n",
    "\n",
    "with open(\"data/english-bad-words-translated-back.txt\") as file:\n",
    "    english_translated_back = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will filter the words that does not translate back to the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = zip(english_original, english_translated, english_translated_back)\n",
    "stable_translations = set([\n",
    "    translation[1] for translation in zipped\n",
    "        if levenshteinDistance(translation[0], translation[2]) < 3\n",
    "        and levenshteinDistance(translation[0], translation[1]) > 3\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stable_translations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m json_object \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mlist\u001b[39m(\u001b[43mstable_translations\u001b[49m), indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/bad_words_translated.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[0;32m      3\u001b[0m     outfile\u001b[38;5;241m.\u001b[39mwrite(json_object)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stable_translations' is not defined"
     ]
    }
   ],
   "source": [
    "json_object = json.dumps(list(stable_translations), indent=4, ensure_ascii=False)\n",
    "with codecs.open(\"data/bad_words_translated.json\", \"w\", \"utf-8\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying bad words from BG Jargon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bgjargon.json', 'r', encoding=\"utf-8\") as f:\n",
    "    dictionary = json.load(f)\n",
    "\n",
    "with open('data/bgjargon_words.json', 'r', encoding=\"utf-8\") as f:\n",
    "    words = set(json.load(f))\n",
    "\n",
    "with open('data/bad_words_translated.json', 'r', encoding=\"utf-8\") as f:\n",
    "    translated_words = set(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all words from bg jargon that have tags - some of the translated bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2714"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_bad_tags = {}\n",
    "\n",
    "for key, value in dictionary.items():\n",
    "    if key in translated_words or any(map(lambda x: x in translated_words, value['tags'])):\n",
    "        dictionary_bad_tags[key] = value\n",
    "\n",
    "len(dictionary_bad_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second iteration - all words that have tags which are already marked as bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3827"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_bad_tags_deep = dictionary_bad_tags.copy()\n",
    "\n",
    "for key, value in dictionary.items():\n",
    "    if any(map(lambda x: x in dictionary_bad_tags.keys(), value['tags'])):\n",
    "        dictionary_bad_tags_deep[key] = value\n",
    "\n",
    "len(dictionary_bad_tags_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same operation as before but equals is replaced with levenshtein distance < 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3501"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_bad_tags_ld = {}\n",
    "\n",
    "for key, value in dictionary.items():\n",
    "    for word in translated_words:\n",
    "        if levenshteinDistance(word, key) < 2 or any(map(lambda x: levenshteinDistance(x, word) < 2, value['tags'])):\n",
    "            dictionary_bad_tags_ld[key] = value\n",
    "            break\n",
    "\n",
    "len(dictionary_bad_tags_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4707"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_bad_tags_deep_ld = dictionary_bad_tags_ld.copy()\n",
    "\n",
    "for key, value in dictionary.items():\n",
    "    if any(map(lambda x: x in dictionary_bad_tags_ld.keys(), value['tags'])):\n",
    "        dictionary_bad_tags_deep_ld[key] = value\n",
    "\n",
    "len(dictionary_bad_tags_deep_ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the classification words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/articles_cleaned_vocabulary.json', 'r', encoding=\"utf-8\") as f:\n",
    "    vocabulary = set([d[0] for d in json.load(f)])\n",
    "\n",
    "\n",
    "def remove_clean_words(bad_words):\n",
    "    return [w for w in bad_words if w not in vocabulary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating single list of bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = list(translated_words)\n",
    "bad_words += remove_clean_words([word.lower() for word in dictionary_bad_tags.keys() if len(word.split(' ')) == 1])\n",
    "\n",
    "json_object = json.dumps(list(set(bad_words)), indent=4, ensure_ascii=False)\n",
    "with codecs.open(\"data/bad_words_1.json\", \"w\", \"utf-8\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "\n",
    "bad_words += remove_clean_words([word.lower() for word in dictionary_bad_tags_ld.keys() if len(word.split(' ')) == 1])\n",
    "json_object = json.dumps(list(set(bad_words)), indent=4, ensure_ascii=False)\n",
    "with codecs.open(\"data/bad_words_2.json\", \"w\", \"utf-8\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "\n",
    "bad_words += remove_clean_words([word.lower() for word in dictionary_bad_tags_deep_ld.keys() if len(word.split(' ')) == 1])\n",
    "json_object = json.dumps(list(set(bad_words)), indent=4, ensure_ascii=False)\n",
    "with codecs.open(\"data/bad_words_3.json\", \"w\", \"utf-8\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
