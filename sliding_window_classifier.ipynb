{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from preprocessing import cyrillize, pattern\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import json\n",
    "import random\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_n_gram(n_gram_vector, examples_vectors) -> bool:\n",
    "    for example_vector in examples_vectors:\n",
    "        # if all items of the example_vector are in n_gram_vector\n",
    "        if all(any(np.array_equal(row, example) for example in n_gram_vector) for row in example_vector):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def split_n_gram(comment, embedding, pad_vector, window_size=5):\n",
    "    comment_vector = embedding(comment)\n",
    "    m = len(comment_vector)\n",
    "    if m == 0:\n",
    "        return []\n",
    "    if m < window_size:\n",
    "        for _ in range(m, window_size):\n",
    "            comment_vector = np.vstack([comment_vector, pad_vector])\n",
    "        m = window_size\n",
    "\n",
    "    i = 0\n",
    "    n_grams = []\n",
    "    while i + window_size <= m:\n",
    "        n_gram = comment_vector[i:i+window_size]\n",
    "        n_grams.append(n_gram)\n",
    "        i += 1\n",
    "    return n_grams\n",
    "\n",
    "def split_and_classify_n_grams(comment_record, embedding, pad_vector, window_size=5):\n",
    "    examples_vectors = [embedding(example) for example in comment_record['examples']] if comment_record['examples'] != None else []\n",
    "\n",
    "    return [{\n",
    "            'n_gram': n_gram,\n",
    "            'label': 'p' if classify_n_gram(n_gram, examples_vectors) else 'n'\n",
    "        }\n",
    "        for n_gram in split_n_gram(comment_record['comment'], embedding, pad_vector, window_size)\n",
    "    ]\n",
    "\n",
    "def predict(model, embedding, comment):\n",
    "    n_grams = split_n_gram(comment, embedding, embedding('[PAD]'))\n",
    "    return any(model.predict(np.concatenate(n_gram).reshape(1, -1)).item() == 1 for n_gram in n_grams)\n",
    "\n",
    "def test_model(model, embedding, testing_set):\n",
    "    tp, fn, fp, tn = 0, 0, 0, 0\n",
    "    for comment in testing_set:\n",
    "        if predict(model, embedding, comment['comment']):\n",
    "            if comment['label'] == 'p':\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if comment['label'] == 'p':\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    return tp, fn, fp, tn\n",
    "\n",
    "def print_test_model(tp, fn, fp, tn):\n",
    "    precision = tp/(tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp/(tp + fn) if tp + fn > 0 else 0\n",
    "    Fscore = (2.0 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    print('Precision: '+str(precision))\n",
    "    print('Recall: '+str(recall))\n",
    "    print('F1-score: '+str(Fscore))\n",
    "    print('Confusion Matrix:')\n",
    "    print('{:15} {:>8} {:>8}'.format('', 'Predicted p', 'Predicted n'))\n",
    "    print('{:15} {:>8.3f} {:>8.3f}'.format('Actual p', tp, fn))\n",
    "    print('{:15} {:>8.3f} {:>8.3f}'.format('Actual n', fp, tn))\n",
    "    return Fscore\n",
    "\n",
    "def train_model(model, train_records, embedding, window_size, balanced_classes, p_n_rate=1.0):\n",
    "    training_n_gram_set = [\n",
    "        (np.concatenate(n['n_gram']), 0 if n['label'] == 'n' else 1)\n",
    "        for comment in train_records\n",
    "        for n in split_and_classify_n_grams(comment, embedding, embedding('[PAD]'), window_size)\n",
    "        if len(n['n_gram']) > 0\n",
    "    ]\n",
    "    positive_train = [a for a in training_n_gram_set if a[1] == 1]\n",
    "    negative_train = [a for a in training_n_gram_set if a[1] == 0]\n",
    "    train_sampled_data = positive_train + negative_train\n",
    "    if balanced_classes:\n",
    "        train_sampled_data = negative_train\n",
    "        train_sampled_data += random.choices(positive_train, k=floor(p_n_rate*len(positive_train)))\n",
    "\n",
    "    train_x, train_y = [a[0] for a in train_sampled_data], [a[1] for a in train_sampled_data]\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "def k_cross_validation(model, supervised_comments, embedding, window_size, k, balanced_classes: bool = False, p_n_rate = 1.0):\n",
    "    n = len(supervised_comments)\n",
    "    m = n//k\n",
    "    t = n//m + n%m\n",
    "\n",
    "    tps, fns, fps, tns = 0, 0, 0, 0\n",
    "\n",
    "    for i in range(0, n, m):\n",
    "        test_records = supervised_comments[i:i+m]\n",
    "        train_records = supervised_comments[0:i] + supervised_comments[i+m:n]\n",
    "\n",
    "        train_model(model, train_records, embedding, window_size, balanced_classes, p_n_rate)\n",
    "\n",
    "        tp, fn, fp, tn = test_model(model, embedding, test_records)\n",
    "        tps += tp\n",
    "        fns += fn\n",
    "        fps += fp\n",
    "        tns += tn\n",
    "\n",
    "    return tps/t, fns/t, fps/t, tns/t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/blitz_comments.json', 'r', encoding=\"utf-8\") as f:\n",
    "    supervised_comments = [\n",
    "        {\n",
    "            'comment': cyrillize(d['comment']),\n",
    "            'label': d['label'],\n",
    "            'examples': d['examples'] if 'examples' in d else None\n",
    "        }\n",
    "        for d in json.load(f) if 'label' in d\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    norm = np.linalg.norm(arr)\n",
    "    return arr/norm if norm != 0 else arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub word tokenization embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_word_tokenization_embedding(dim=100, norm=True):\n",
    "    tokenizer = Tokenizer.from_file(\"data/tokenizer_comments.json\")\n",
    "    token2ind = tokenizer.get_vocab()\n",
    "    ind2token = lambda x: tokenizer.id_to_token(x)\n",
    "\n",
    "    with open('data/unsupervised_comments.json', 'r', encoding=\"utf-8\") as f:\n",
    "        unsupervised_comments = json.load(f)\n",
    "\n",
    "    tokenized_unsupervised_comments = [tokenizer.encode(c).tokens for c in unsupervised_comments]\n",
    "\n",
    "    n_words = tokenizer.get_vocab_size()\n",
    "    X=np.zeros((n_words,n_words))\n",
    "    for s in [\"[UNK]\", \"[PAD]\", \"[STR]\", \"[END]\"]:\n",
    "        X[token2ind[s], token2ind[s]] = 1\n",
    "    for comment in tokenized_unsupervised_comments:\n",
    "        for wi in range(len(comment)):\n",
    "            if comment[wi] not in token2ind: continue\n",
    "            i=token2ind[comment[wi]]\n",
    "            for k in range(1,4+1):\n",
    "                if wi-k>=0 and comment[wi-k] in token2ind:\n",
    "                    j=token2ind[comment[wi-k]]\n",
    "                    X[i,j] += 1\n",
    "                if wi+k<len(comment) and comment[wi+k] in token2ind:\n",
    "                    j=token2ind[comment[wi+k]]\n",
    "                    X[i,j] += 1\n",
    "\n",
    "    svd = TruncatedSVD(n_components=dim, n_iter=10)\n",
    "    svd.fit(X)\n",
    "    X_reduced = svd.transform(X)\n",
    "\n",
    "    return lambda comment: np.stack([normalize(X_reduced[token2ind[token]]) for token in tokenizer.encode(comment).tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise dampening embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_dampening_embedding(dim, device):\n",
    "    class EncoderRNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "            super(EncoderRNN, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "\n",
    "            self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "            self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        def forward(self, input):\n",
    "            embedded = self.dropout(self.embedding(input))\n",
    "            output, hidden = self.gru(embedded)\n",
    "            return output, hidden\n",
    "\n",
    "        def save(self, filepath):\n",
    "            torch.save(self.state_dict(), filepath)\n",
    "\n",
    "        @classmethod\n",
    "        def load(cls, filepath, input_size, hidden_size, dropout_p=0.1):\n",
    "            model = cls(input_size, hidden_size, dropout_p)\n",
    "            model.load_state_dict(torch.load(filepath))\n",
    "            return model\n",
    "\n",
    "    SOW_token = '\u0002'\n",
    "    EOW_token = '\u0003'\n",
    "    UNK_token = '�'\n",
    "\n",
    "    alphabet_for_generation = 'абвгдежзийклмнопрстуфхцчшщьъюяabcdefghijklmnopqrstuvwxyz!@#$%^&*()-_=+[]\\';.,/`~\"<>|1234567890'\n",
    "    alphabet = alphabet_for_generation\n",
    "    alphabet += SOW_token\n",
    "    alphabet += EOW_token\n",
    "    alphabet += UNK_token\n",
    "\n",
    "    char2ind = {}\n",
    "    for i, c in enumerate(alphabet):\n",
    "        char2ind[c] = i\n",
    "\n",
    "    def indexesFromWord(word):\n",
    "        return [(char2ind[c] if c in char2ind else char2ind[UNK_token]) for c in word]\n",
    "\n",
    "    def tensorFromWord(word):\n",
    "        indexes = indexesFromWord(word)\n",
    "        indexes.append(char2ind[EOW_token])\n",
    "        return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "    encoder = EncoderRNN.load(\"data/embedding_encoder_100_000_smaller_alphabet.pth\", 96, 128)\n",
    "    encoder.to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "    def embedding(word):\n",
    "        if word == '[PAD]':\n",
    "            return torch.zeros(1, 1, 128)\n",
    "        with torch.no_grad():\n",
    "            input_tensor = tensorFromWord(word)\n",
    "            _, encoder_hidden = encoder(input_tensor)\n",
    "        return encoder_hidden\n",
    "\n",
    "    # reducing dims with svd on the unsupervised comments\n",
    "    with open('data/unsupervised_comments.json', 'r', encoding=\"utf-8\") as f:\n",
    "        unsupervised_comments = json.load(f)\n",
    "    vocabulary = set([t for comment in unsupervised_comments for t in regexp_tokenize(comment, pattern)])\n",
    "    X = np.vstack([embedding(t).cpu().flatten() for t in vocabulary])\n",
    "    svd = TruncatedSVD(n_components=dim, n_iter=10)\n",
    "    svd.fit(X)\n",
    "\n",
    "    def comment_embedding(comment):\n",
    "        tokens = [\n",
    "            normalize(svd.transform(embedding(word).cpu().flatten(end_dim=1)).flatten())\n",
    "            for word in regexp_tokenize(comment, pattern)\n",
    "        ]\n",
    "        if len(tokens) != 0:\n",
    "            return np.vstack(tokens)\n",
    "        return []\n",
    "\n",
    "    return comment_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_word_embedding = get_sub_word_tokenization_embedding(dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dampening_embedding = get_noise_dampening_embedding(dim=100, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boosted stumps with sub word embedding\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient boosted stumps with sub word embedding\")\n",
    "print_test_model(*k_cross_validation(clf, supervised_comments, sub_word_embedding, window_size=5, k=10, balanced_classes=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boosted stumps with ND embedding\n",
      "Precision: 0.5535714285714286\n",
      "Recall: 0.037851037851037855\n",
      "F1-score: 0.07085714285714287\n",
      "Confusion Matrix:\n",
      "                Predicted p Predicted n\n",
      "Actual p           1.824   46.353\n",
      "Actual n           1.471  131.353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07085714285714287"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Gradient boosted stumps with ND embedding\")\n",
    "print_test_model(*k_cross_validation(clf, supervised_comments, noise_dampening_embedding, window_size=5, k=10, balanced_classes=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5135135135135135\n",
      "Recall: 0.336283185840708\n",
      "F1-score: 0.40641711229946526\n",
      "Confusion Matrix:\n",
      "                Predicted p Predicted n\n",
      "Actual p          38.000   75.000\n",
      "Actual n          36.000  158.000\n",
      "Precision: 0.5925925925925926\n",
      "Recall: 0.2831858407079646\n",
      "F1-score: 0.38323353293413176\n",
      "Confusion Matrix:\n",
      "                Predicted p Predicted n\n",
      "Actual p          32.000   81.000\n",
      "Actual n          22.000  172.000\n",
      "Precision: 0.44\n",
      "Recall: 0.19469026548672566\n",
      "F1-score: 0.26993865030674846\n",
      "Confusion Matrix:\n",
      "                Predicted p Predicted n\n",
      "Actual p          22.000   91.000\n",
      "Actual n          28.000  166.000\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.10619469026548672\n",
      "F1-score: 0.183206106870229\n",
      "Confusion Matrix:\n",
      "                Predicted p Predicted n\n",
      "Actual p          12.000  101.000\n",
      "Actual n           6.000  188.000\n"
     ]
    }
   ],
   "source": [
    "best_i, best_f = 1, 0\n",
    "for i in range(1, 8, 2):\n",
    "    j = len(supervised_comments)//10\n",
    "    test_records = supervised_comments[0:j]\n",
    "    train_records = supervised_comments[j:]\n",
    "    model = KNeighborsClassifier(i)\n",
    "    train_model(model, train_records, sub_word_embedding, window_size=5, balanced_classes=False)\n",
    "\n",
    "    f = print_test_model(*test_model(model, sub_word_embedding, test_records))\n",
    "    if f > best_f:\n",
    "        best_f = f\n",
    "        best_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-NN with sub word embedding\n",
      "Precision: 0.391820580474934\n",
      "Recall: 0.3626373626373626\n",
      "F1-score: 0.3766645529486366\n",
      "Confusion Matrix:\n",
      "                Predicted p Predicted n\n",
      "Actual p          17.471   30.706\n",
      "Actual n          27.118  105.706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3766645529486366"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"K-NN with sub word embedding\")\n",
    "print_test_model(*k_cross_validation(KNeighborsClassifier(best_i), supervised_comments, sub_word_embedding, window_size=5, k=10, balanced_classes=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.45454545454545453\n",
      "Recall: 0.35398230088495575\n",
      "F1-score: 0.39800995024875624\n",
      "Confusion Matrix:\n",
      "                Predicted p Predicted n\n",
      "Actual p          40.000   73.000\n",
      "Actual n          48.000  146.000\n",
      "Precision: 0.4691358024691358\n",
      "Recall: 0.336283185840708\n",
      "F1-score: 0.3917525773195876\n",
      "Confusion Matrix:\n",
      "                Predicted p Predicted n\n",
      "Actual p          38.000   75.000\n",
      "Actual n          43.000  151.000\n",
      "Precision: 0.36507936507936506\n",
      "Recall: 0.20353982300884957\n",
      "F1-score: 0.26136363636363635\n",
      "Confusion Matrix:\n",
      "                Predicted p Predicted n\n",
      "Actual p          23.000   90.000\n",
      "Actual n          40.000  154.000\n",
      "Precision: 0.4838709677419355\n",
      "Recall: 0.13274336283185842\n",
      "F1-score: 0.20833333333333334\n",
      "Confusion Matrix:\n",
      "                Predicted p Predicted n\n",
      "Actual p          15.000   98.000\n",
      "Actual n          16.000  178.000\n"
     ]
    }
   ],
   "source": [
    "best_i, best_f = 1, 0.0\n",
    "for i in range(1, 8, 2):\n",
    "    j = len(supervised_comments)//10\n",
    "    test_records = supervised_comments[0:j]\n",
    "    train_records = supervised_comments[j:]\n",
    "    model = KNeighborsClassifier(i)\n",
    "    train_model(model, train_records, noise_dampening_embedding, window_size=5, balanced_classes=False)\n",
    "\n",
    "    f = print_test_model(*test_model(model, noise_dampening_embedding, test_records))\n",
    "    if f > best_f:\n",
    "        best_f = f\n",
    "        best_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-NN with ND embedding\n",
      "Precision: 0.3282520325203252\n",
      "Recall: 0.3943833943833944\n",
      "F1-score: 0.35829173599556297\n",
      "Confusion Matrix:\n",
      "                Predicted p Predicted n\n",
      "Actual p          19.000   29.176\n",
      "Actual n          38.882   93.941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35829173599556297"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"K-NN with ND embedding\")\n",
    "print_test_model(*k_cross_validation(KNeighborsClassifier(best_i), supervised_comments, noise_dampening_embedding, window_size=5, k=10, balanced_classes=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP with sub word embedding\")\n",
    "print_test_model(*k_cross_validation(MLPClassifier((500, 500, 2), activation='relu'), supervised_comments, sub_word_embedding, window_size=5, k=10, balanced_classes=True, p_n_rate=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLP with ND embedding\")\n",
    "print_test_model(*k_cross_validation(MLPClassifier((500, 500, 2), activation='relu'), supervised_comments, noise_dampening_embedding, window_size=5, k=10, balanced_classes=True, p_n_rate=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVC with sub word embedding\")\n",
    "print_test_model(*k_cross_validation(svm.SVC(), supervised_comments, sub_word_embedding, window_size=5, k=10, balanced_classes=True, p_n_rate=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVC with ND embedding\")\n",
    "print_test_model(*k_cross_validation(svm.SVC(), supervised_comments, noise_dampening_embedding, window_size=5, k=10, balanced_classes=True, p_n_rate=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
