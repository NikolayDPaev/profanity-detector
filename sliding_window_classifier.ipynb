{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "from preprocessing import cyrillize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import json\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/blitz_comments.json', 'r', encoding=\"utf-8\") as f:\n",
    "    supervised_comments = [\n",
    "        {\n",
    "            'comment': cyrillize(d['comment']),\n",
    "            'label': d['label'],\n",
    "            'examples': d['examples'] if 'examples' in d else None\n",
    "        }\n",
    "        for d in json.load(f) if 'label' in d\n",
    "    ]\n",
    "\n",
    "n_supervised_p = len([s for s in supervised_comments if s['label'] == 'p'])\n",
    "n_supervised_n = len([s for s in supervised_comments if s['label'] == 'p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub word tokenization embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_word_tokenization_embedding(dim=100):\n",
    "    tokenizer = Tokenizer.from_file(\"data/tokenizer_comments.json\")\n",
    "    token2ind = tokenizer.get_vocab()\n",
    "    ind2token = lambda x: tokenizer.id_to_token(x)\n",
    "\n",
    "    with open('data/unsupervised_comments.json', 'r', encoding=\"utf-8\") as f:\n",
    "        unsupervised_comments = json.load(f)\n",
    "\n",
    "    tokenized_unsupervised_comments = [tokenizer.encode(c).tokens for c in unsupervised_comments]\n",
    "\n",
    "    n_words = tokenizer.get_vocab_size()\n",
    "    X=np.zeros((n_words,n_words))\n",
    "    for s in [\"[UNK]\", \"[PAD]\", \"[STR]\", \"[END]\"]:\n",
    "        X[token2ind[s], token2ind[s]] = 1\n",
    "    for comment in tokenized_unsupervised_comments:\n",
    "        for wi in range(len(comment)):\n",
    "            if comment[wi] not in token2ind: continue\n",
    "            i=token2ind[comment[wi]]\n",
    "            for k in range(1,4+1):\n",
    "                if wi-k>=0 and comment[wi-k] in token2ind:\n",
    "                    j=token2ind[comment[wi-k]]\n",
    "                    X[i,j] += 1\n",
    "                if wi+k<len(comment) and comment[wi+k] in token2ind:\n",
    "                    j=token2ind[comment[wi+k]]\n",
    "                    X[i,j] += 1\n",
    "\n",
    "    svd = TruncatedSVD(n_components=dim, n_iter=10)\n",
    "    svd.fit(X)\n",
    "    X_reduced = svd.transform(X)\n",
    "\n",
    "    return lambda comment: np.stack([X_reduced[token2ind[token]] for token in tokenizer.encode(comment).tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise dampening embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sliding window classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, window_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Linear(input_size, hidden_size)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.hidden = nn.Linear(window_size*hidden_size, 2)\n",
    "        self.dist = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = [self.weight(torch.tensor(n_gram, dtype=torch.float32)) for n_gram in x]\n",
    "        weights = torch.cat(weights)\n",
    "        hidden = self.hidden(weights)\n",
    "        return self.dist(hidden)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            outputs = self(x)\n",
    "            return np.argmax(outputs.numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_grams, input_size, hidden_size, window_size, epochs=1):\n",
    "    model = FastTextClassifier(input_size, hidden_size, window_size)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    current_loss = 0.0\n",
    "    for e in range(epochs):\n",
    "        for i, (x, y) in enumerate(n_grams):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prediction = model(x)\n",
    "            print(prediction, torch.tensor(y))\n",
    "            loss = loss_function(prediction, torch.tensor(y))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            current_loss += loss.item()\n",
    "\n",
    "            if i%100 == 0:\n",
    "                print(f'Average loss after batch %d: %.3f'%((i/100)+1, current_loss/100))\n",
    "                current_loss = 0.0\n",
    "        print(f'Epoch {e+1} finished')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_n_gram(n_gram_vector, examples_vectors) -> bool:\n",
    "    for example_vector in examples_vectors:\n",
    "        # if all items of the example_vector are in n_gram_vector\n",
    "        if all(any(np.array_equal(item, example) for example in n_gram_vector) for item in example_vector):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def split_n_gram(comment, embedding, pad_vector, window_size=5):\n",
    "    comment_vector = embedding(comment)\n",
    "    m = len(comment_vector)\n",
    "    if m < window_size:\n",
    "        for _ in range(m, window_size):\n",
    "            comment_vector = np.vstack([comment_vector, pad_vector])\n",
    "        m = window_size\n",
    "\n",
    "    i = 0\n",
    "    n_grams = []\n",
    "    while i + window_size <= m:\n",
    "        n_gram = comment_vector[i:i+window_size]\n",
    "        n_grams.append(n_gram)\n",
    "        i += 1\n",
    "    return n_grams\n",
    "\n",
    "def split_and_classify_n_grams(comment_record, embedding, pad_vector, window_size=5):\n",
    "    examples_vectors = [embedding(example) for example in comment_record['examples']] if comment_record['examples'] != None else []\n",
    "\n",
    "    return [{\n",
    "            'n_gram': n_gram,\n",
    "            'label': 'p' if classify_n_gram(n_gram, examples_vectors) else 'n'\n",
    "        }\n",
    "        for n_gram in split_n_gram(comment_record['comment'], embedding, pad_vector, window_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(comments, test_fraction = 0.1):\n",
    "    random.seed(42)\n",
    "    random.shuffle(comments)\n",
    "    test_count = int(len(comments) * test_fraction)\n",
    "    test_comments = comments[:test_count]\n",
    "    train_comments = comments[test_count:]\n",
    "    return test_comments, train_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training FastText with sub word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set, training_set = split_test_train(supervised_comments)\n",
    "\n",
    "input_size = 100\n",
    "embedding = get_sub_word_tokenization_embedding(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "\n",
    "training_n_gram_set = [\n",
    "        (n['n_gram'], 0 if n['label'] == 'n' else 1)\n",
    "        for comment in training_set\n",
    "        for n in split_and_classify_n_grams(comment, embedding, embedding('[PAD]'), window_size)\n",
    "    ]\n",
    "\n",
    "test_n_gram_set = [\n",
    "        (n['n_gram'], 0 if n['label'] == 'n' else 1)\n",
    "        for comment in testing_set\n",
    "        for n in split_and_classify_n_grams(comment, embedding, embedding('[PAD]'), window_size)\n",
    "]\n",
    "\n",
    "model = train_model(training_n_gram_set, input_size, input_size, window_size, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2798, 0.7202], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.003\n",
      "tensor([0.8113, 0.1887], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 3.4538e-06], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 1 finished\n",
      "tensor([0.7790, 0.2210], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.016\n",
      "tensor([9.9963e-01, 3.7247e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 6.9452e-09], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 2 finished\n",
      "tensor([0.9494, 0.0506], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.019\n",
      "tensor([9.9999e-01, 6.5225e-06], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 1.1897e-10], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 3 finished\n",
      "tensor([0.9857, 0.0143], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 3.6712e-07], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 6.4892e-12], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 4 finished\n",
      "tensor([0.9946, 0.0054], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 4.4119e-08], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 7.5708e-13], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 5 finished\n",
      "tensor([0.9975, 0.0025], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 9.0257e-09], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 1.5109e-13], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 6 finished\n",
      "tensor([0.9986, 0.0014], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 2.7209e-09], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 4.4667e-14], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 7 finished\n",
      "tensor([9.9910e-01, 9.0180e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 1.0947e-09], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 1.7708e-14], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 8 finished\n",
      "tensor([9.9936e-01, 6.3922e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.4751e-10], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.7606e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 9 finished\n",
      "tensor([9.9951e-01, 4.9032e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 3.2299e-10], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 5.1276e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 10 finished\n",
      "tensor([9.9960e-01, 3.9967e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 2.1606e-10], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 3.4109e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 11 finished\n",
      "tensor([9.9966e-01, 3.4135e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 1.5907e-10], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 2.5012e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 12 finished\n",
      "tensor([9.9970e-01, 3.0219e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 1.2597e-10], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 1.9755e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 13 finished\n",
      "tensor([9.9973e-01, 2.7497e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 1.0547e-10], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 1.6509e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 14 finished\n",
      "tensor([9.9974e-01, 2.5553e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 9.2110e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 1.4401e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 15 finished\n",
      "tensor([9.9976e-01, 2.4129e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 8.3066e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 1.2977e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 16 finished\n",
      "tensor([9.9977e-01, 2.3065e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 7.6752e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 1.1986e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 17 finished\n",
      "tensor([9.9978e-01, 2.2253e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 7.2233e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 1.1278e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 18 finished\n",
      "tensor([9.9978e-01, 2.1620e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 6.8929e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 1.0762e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 19 finished\n",
      "tensor([9.9979e-01, 2.1117e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 6.6466e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 1.0378e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 20 finished\n",
      "tensor([9.9979e-01, 2.0708e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 6.4596e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 1.0088e-15], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 21 finished\n",
      "tensor([9.9980e-01, 2.0367e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 6.3149e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 9.8635e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 22 finished\n",
      "tensor([9.9980e-01, 2.0077e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 6.2006e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 9.6875e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 23 finished\n",
      "tensor([9.9980e-01, 1.9824e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 6.1085e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 9.5461e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 24 finished\n",
      "tensor([9.9980e-01, 1.9597e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 6.0325e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 9.4300e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 25 finished\n",
      "tensor([9.9981e-01, 1.9391e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.9682e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 9.3324e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 26 finished\n",
      "tensor([9.9981e-01, 1.9199e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.9126e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 9.2483e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 27 finished\n",
      "tensor([9.9981e-01, 1.9018e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.8633e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 9.1742e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 28 finished\n",
      "tensor([9.9981e-01, 1.8846e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.8186e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 9.1074e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 29 finished\n",
      "tensor([9.9981e-01, 1.8678e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.7774e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 9.0458e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 30 finished\n",
      "tensor([9.9981e-01, 1.8516e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.7386e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.9882e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 31 finished\n",
      "tensor([9.9982e-01, 1.8356e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.7016e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.9334e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 32 finished\n",
      "tensor([9.9982e-01, 1.8199e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.6660e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.8806e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 33 finished\n",
      "tensor([9.9982e-01, 1.8044e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.6313e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.8293e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 34 finished\n",
      "tensor([9.9982e-01, 1.7890e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.5973e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.7792e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 35 finished\n",
      "tensor([9.9982e-01, 1.7737e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.5639e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.7298e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 36 finished\n",
      "tensor([9.9982e-01, 1.7585e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.5308e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.6812e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 37 finished\n",
      "tensor([9.9983e-01, 1.7434e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.4980e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.6329e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 38 finished\n",
      "tensor([9.9983e-01, 1.7284e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.4655e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.5849e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 39 finished\n",
      "tensor([9.9983e-01, 1.7135e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.4331e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.5371e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 40 finished\n",
      "tensor([9.9983e-01, 1.6986e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.4008e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.4896e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 41 finished\n",
      "tensor([9.9983e-01, 1.6837e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.3687e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.4422e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 42 finished\n",
      "tensor([9.9983e-01, 1.6690e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.3367e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.3950e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 43 finished\n",
      "tensor([9.9983e-01, 1.6543e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.3048e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.3479e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 44 finished\n",
      "tensor([9.9984e-01, 1.6396e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.2729e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.3010e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 45 finished\n",
      "tensor([9.9984e-01, 1.6251e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.2412e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.2541e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 46 finished\n",
      "tensor([9.9984e-01, 1.6106e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.2095e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.2074e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 47 finished\n",
      "tensor([9.9984e-01, 1.5962e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.1780e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.1608e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 48 finished\n",
      "tensor([9.9984e-01, 1.5819e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.1465e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.1144e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 49 finished\n",
      "tensor([9.9984e-01, 1.5677e-04], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "Average loss after batch 1: -0.020\n",
      "tensor([1.0000e+00, 5.1152e-11], grad_fn=<SoftmaxBackward0>) tensor(0)\n",
      "tensor([1.0000e+00, 8.0681e-16], grad_fn=<SoftmaxBackward0>) tensor(1)\n",
      "Epoch 50 finished\n"
     ]
    }
   ],
   "source": [
    "window_size = 5\n",
    "\n",
    "training_set = [{\n",
    "            'comment': 'тъпанари',\n",
    "            'examples': None\n",
    "        },{\n",
    "            'comment': 'круши, круши, круши',\n",
    "            'examples': ['круши']\n",
    "        }]\n",
    "\n",
    "training_n_gram_set = [\n",
    "        (n['n_gram'], 0 if n['label'] == 'n' else 1)\n",
    "        for comment in training_set\n",
    "        for n in split_and_classify_n_grams(comment, embedding, embedding('[PAD]'), window_size)\n",
    "    ]\n",
    "\n",
    "test_n_gram_set = [\n",
    "        (n['n_gram'], 0 if n['label'] == 'n' else 1)\n",
    "        for comment in testing_set\n",
    "        for n in split_and_classify_n_grams(comment, embedding, embedding('[PAD]'), window_size)\n",
    "]\n",
    "\n",
    "model = train_model(training_n_gram_set, input_size, input_size, window_size, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing FastText with sub word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, comment):\n",
    "    n_grams = split_n_gram(comment, embedding, embedding('[PAD]'))\n",
    "    return any(model.predict(n_gram) for n_gram in n_grams)\n",
    "\n",
    "def test_model(model, testing_set):\n",
    "    p_class = [c['comment'] for c in testing_set if c['label'] == 'p']\n",
    "    n_class = [c['comment'] for c in testing_set if c['label'] == 'n']\n",
    "    test_classes = [p_class, n_class]\n",
    "\n",
    "    confusionMatrix = [[0, 0], [0, 0]]\n",
    "    for c in range(2):\n",
    "        for comment in test_classes[c]:\n",
    "            c_MAP = predict(model, comment)\n",
    "            confusionMatrix[c][c_MAP] += 1\n",
    "\n",
    "    sum_positive = sum(confusionMatrix[x][0] for x in range(2))\n",
    "    precision = confusionMatrix[0][0] / sum_positive\n",
    "    recall = confusionMatrix[0][0] / len(p_class)\n",
    "    Fscore = (2.0 * precision * recall) / (precision + recall)\n",
    "    print('=================================================================')\n",
    "    print('Confusion matrix: ')\n",
    "    for row in confusionMatrix:\n",
    "        for val in row:\n",
    "            print('{:4}'.format(val), end = '')\n",
    "        print()\n",
    "    print('Precision: '+str(precision))\n",
    "    print('Recall: '+str(recall))\n",
    "    print('F1-score: '+str(Fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Confusion matrix: \n",
      "  76   0\n",
      " 204   0\n",
      "Precision: 0.2714285714285714\n",
      "Recall: 1.0\n",
      "F1-score: 0.42696629213483145\n"
     ]
    }
   ],
   "source": [
    "test_model(model, testing_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
