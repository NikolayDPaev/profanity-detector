{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from preprocessing import cyrillize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "from math import floor\n",
    "from get_embeddings import get_sub_word_tokenization_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_n_gram(n_gram_vector, examples_vectors) -> bool:\n",
    "    for example_vector in examples_vectors:\n",
    "        # if all items of the example_vector are in n_gram_vector\n",
    "        if all(any(np.array_equal(row, example) for example in n_gram_vector) for row in example_vector):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def split_n_gram(comment, embedding, pad_vector, window_size=5):\n",
    "    comment_vector = embedding(cyrillize(comment))\n",
    "    m = len(comment_vector)\n",
    "    if m == 0:\n",
    "        return []\n",
    "    if m < window_size:\n",
    "        for _ in range(m, window_size):\n",
    "            comment_vector = np.vstack([comment_vector, pad_vector])\n",
    "        m = window_size\n",
    "\n",
    "    i = 0\n",
    "    n_grams = []\n",
    "    while i + window_size <= m:\n",
    "        n_gram = comment_vector[i:i+window_size]\n",
    "        n_grams.append(n_gram)\n",
    "        i += 1\n",
    "    return n_grams\n",
    "\n",
    "def split_and_classify_n_grams(comment_record, embedding, pad_vector, window_size=5):\n",
    "    examples_vectors = [embedding(example) for example in comment_record['examples']] if 'examples' in comment_record != None else []\n",
    "\n",
    "    return [{\n",
    "            'n_gram': n_gram,\n",
    "            'label': 'p' if classify_n_gram(n_gram, examples_vectors) else 'n'\n",
    "        }\n",
    "        for n_gram in split_n_gram(comment_record['comment'], embedding, pad_vector, window_size)\n",
    "    ]\n",
    "\n",
    "def predict(model, embedding, comment):\n",
    "    n_grams = split_n_gram(comment, embedding, embedding('[PAD]'))\n",
    "    return any(model.predict(np.concatenate(n_gram).reshape(1, -1)).item() == 1 for n_gram in n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_records, embedding, window_size, balanced_classes, p_n_rate=1.0):\n",
    "    training_n_gram_set = [\n",
    "        (np.concatenate(n['n_gram']), 0 if n['label'] == 'n' else 1)\n",
    "        for comment in train_records\n",
    "        for n in split_and_classify_n_grams(comment, embedding, embedding('[PAD]'), window_size)\n",
    "        if len(n['n_gram']) > 0\n",
    "    ]\n",
    "    positive_train = [a for a in training_n_gram_set if a[1] == 1]\n",
    "    negative_train = [a for a in training_n_gram_set if a[1] == 0]\n",
    "    train_sampled_data = positive_train + negative_train\n",
    "    if balanced_classes:\n",
    "        train_sampled_data = negative_train\n",
    "        train_sampled_data += random.choices(positive_train, k=floor(p_n_rate*len(positive_train)))\n",
    "\n",
    "    train_x, train_y = [a[0] for a in train_sampled_data], [a[1] for a in train_sampled_data]\n",
    "    model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_word_tokenization_embedding(dim=100, norm=True):\n",
    "    tokenizer = Tokenizer.from_file(\"data/tokenizer_comments.json\")\n",
    "    token2ind = tokenizer.get_vocab()\n",
    "    ind2token = lambda x: tokenizer.id_to_token(x)\n",
    "\n",
    "    with open('data/unsupervised_comments.json', 'r', encoding=\"utf-8\") as f:\n",
    "        unsupervised_comments = json.load(f)\n",
    "\n",
    "    tokenized_unsupervised_comments = [tokenizer.encode(c.lower()).tokens for c in unsupervised_comments]\n",
    "\n",
    "    n_words = tokenizer.get_vocab_size()\n",
    "    X=np.zeros((n_words,n_words))\n",
    "    for s in [\"[UNK]\", \"[PAD]\", \"[STR]\", \"[END]\"]:\n",
    "        X[token2ind[s], token2ind[s]] = 1\n",
    "    for comment in tokenized_unsupervised_comments:\n",
    "        for wi in range(len(comment)):\n",
    "            if comment[wi] not in token2ind: continue\n",
    "            i=token2ind[comment[wi]]\n",
    "            for k in range(1,4+1):\n",
    "                if wi-k>=0 and comment[wi-k] in token2ind:\n",
    "                    j=token2ind[comment[wi-k]]\n",
    "                    X[i,j] += 1\n",
    "                if wi+k<len(comment) and comment[wi+k] in token2ind:\n",
    "                    j=token2ind[comment[wi+k]]\n",
    "                    X[i,j] += 1\n",
    "\n",
    "    svd = TruncatedSVD(n_components=dim, n_iter=10)\n",
    "    svd.fit(X)\n",
    "    X_reduced = svd.transform(X)\n",
    "\n",
    "    return lambda comment: np.stack([X_reduced[token2ind[token]] for token in tokenizer.encode(comment.lower()).tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/blitz_comments.json', 'r', encoding=\"utf-8\") as f:\n",
    "    comments = json.load(f)\n",
    "supervised_comments = [\n",
    "    d for d in comments if 'label' in d\n",
    "]\n",
    "unsupervised_comments = [\n",
    "    d for d in comments if 'label' not in d\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_word_embedding = get_sub_word_tokenization_embedding(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(probability=True)\n",
    "\n",
    "train_model(svc, supervised_comments, sub_word_embedding, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_entropy_in_comment(comment):\n",
    "    n_grams = np.array([np.concatenate(n) for n in split_n_gram(comment['comment'], sub_word_embedding, sub_word_embedding('[PAD]'), 5)])\n",
    "    ps = svc.predict_proba(n_grams)\n",
    "    return max([-p[0]*math.log(p[0]) - p[1]*math.log(p[1]) for p in ps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_comments_tuple = [(c, max_entropy_in_comment(c)) for c in unsupervised_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_unsupervised_comments = [x[0] for x in sorted(unsupervised_comments_tuple, key= lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_unsupervised_comments[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment': 'Праз. Е и какво от това?', 'author': 'Голям'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_unsupervised_comments[-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_unsupervised_comments = list(reversed(sorted_unsupervised_comments))\n",
    "sorted_comments = supervised_comments + sorted_unsupervised_comments\n",
    "\n",
    "import codecs\n",
    "\n",
    "json_object = json.dumps(sorted_comments, indent=4, ensure_ascii=False)\n",
    "with codecs.open(\"data/blitz_comments_sorted.json\", \"w\", \"utf-8\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
