{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspell import SymSpell\n",
    "\n",
    "import json\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the symspell dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/articles_cleaned_vocabulary.json', 'r', encoding=\"utf-8\") as f:\n",
    "    vocabulary = set([d[0] for d in json.load(f)])\n",
    "\n",
    "pattern = r'([0-9]+|[а-я]+)'\n",
    "with open('data/bgjargon_words.json', 'r', encoding=\"utf-8\") as f:\n",
    "    vocabulary.union(set([token for word in json.load(f) for token in regexp_tokenize(word, pattern)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bad_words_translated.json', 'r', encoding=\"utf-8\") as f:\n",
    "    bad_words_original = set(json.load(f))\n",
    "\n",
    "with open('data/bad_words_1.json', 'r', encoding=\"utf-8\") as f:\n",
    "    bad_words_bgjargon_1 = set(json.load(f))\n",
    "\n",
    "with open('data/bad_words_2.json', 'r', encoding=\"utf-8\") as f:\n",
    "    bad_words_bgjargon_2 = set(json.load(f))\n",
    "\n",
    "with open('data/bad_words_3.json', 'r', encoding=\"utf-8\") as f:\n",
    "    bad_words_bgjargon_3 = set(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the gold comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/blitz_comments_classified.json', 'r', encoding=\"utf-8\") as f:\n",
    "    gold_comments = [comment_record for comment_record in json.load(f) if len(comment_record) > 2]\n",
    "\n",
    "gold_comments_p = [c for c in gold_comments if c[2] == 'p']\n",
    "gold_comments_n = [c for c in gold_comments if c[2] == 'n']\n",
    "\n",
    "min_len = min(len(gold_comments_p), len(gold_comments_n))\n",
    "\n",
    "gold_comments_sample = gold_comments_p[:min_len] + gold_comments_n[:min_len]\n",
    "random.shuffle(gold_comments_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification after correction with symspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_of_word_similarity_annotation_with_symspell_suggestions(max_edit_distance, weight_of_bad, bad_words) -> tuple[float, float]:\n",
    "    ss = SymSpell(max_dictionary_edit_distance=max_edit_distance)\n",
    "    for word in vocabulary:\n",
    "        ss._create_dictionary_entry(word, 1)\n",
    "    if weight_of_bad:\n",
    "        for word in bad_words:\n",
    "            ss._create_dictionary_entry(word, 1)\n",
    "\n",
    "    def auto_classification_as_p(comment) -> bool:\n",
    "        for token in ss.lookup_compound(comment[0], max_edit_distance)[0].term.split(\" \"):\n",
    "            if token in bad_words:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    tp, fn, fp = 0, 0, 0\n",
    "    for comment in gold_comments_sample:\n",
    "        if auto_classification_as_p(comment):\n",
    "            if comment[2] == 'p':\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if comment[2] == 'p':\n",
    "                fn += 1\n",
    "    return tp/(tp + fp), tp/(tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad words set=1 max edit distance=1 weight=True -> (0.4352078239608802, 0.967391304347826, 0.2807570977917981)\n",
      "bad words set=2 max edit distance=1 weight=True -> (0.5427927927927928, 0.9488188976377953, 0.3801261829652997)\n",
      "bad words set=3 max edit distance=1 weight=True -> (0.5764192139737991, 0.9361702127659575, 0.416403785488959)\n",
      "bad words set=4 max edit distance=1 weight=True -> (0.575107296137339, 0.8993288590604027, 0.4227129337539432)\n",
      "bad words set=1 max edit distance=2 weight=True -> (0.4357405140758873, 0.9726775956284153, 0.2807570977917981)\n",
      "bad words set=2 max edit distance=2 weight=True -> (0.5614035087719299, 0.920863309352518, 0.4037854889589905)\n",
      "bad words set=3 max edit distance=2 weight=True -> (0.5944798301486198, 0.9090909090909091, 0.4416403785488959)\n",
      "bad words set=4 max edit distance=2 weight=True -> (0.5898234683281411, 0.8632218844984803, 0.4479495268138801)\n",
      "bad words set=1 max edit distance=3 weight=True -> (0.45622119815668205, 0.8461538461538461, 0.31230283911671924)\n",
      "bad words set=2 max edit distance=3 weight=True -> (0.5621052631578948, 0.8449367088607594, 0.42113564668769715)\n",
      "bad words set=3 max edit distance=3 weight=True -> (0.5967413441955193, 0.8419540229885057, 0.46214511041009465)\n",
      "bad words set=4 max edit distance=3 weight=True -> (0.6016096579476862, 0.8305555555555556, 0.471608832807571)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bad words set no.</th>\n",
       "      <th>Max dictionary edit distance</th>\n",
       "      <th>More weight of bad words</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.435208</td>\n",
       "      <td>0.967391</td>\n",
       "      <td>0.280757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.542793</td>\n",
       "      <td>0.948819</td>\n",
       "      <td>0.380126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.576419</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.416404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.575107</td>\n",
       "      <td>0.899329</td>\n",
       "      <td>0.422713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.435741</td>\n",
       "      <td>0.972678</td>\n",
       "      <td>0.280757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>0.403785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.594480</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.441640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.589823</td>\n",
       "      <td>0.863222</td>\n",
       "      <td>0.447950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.456221</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.312303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.562105</td>\n",
       "      <td>0.844937</td>\n",
       "      <td>0.421136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.596741</td>\n",
       "      <td>0.841954</td>\n",
       "      <td>0.462145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.601610</td>\n",
       "      <td>0.830556</td>\n",
       "      <td>0.471609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Bad words set no.  Max dictionary edit distance  More weight of bad words  \\\n",
       "0                   1                             1                      True   \n",
       "1                   2                             1                      True   \n",
       "2                   3                             1                      True   \n",
       "3                   4                             1                      True   \n",
       "4                   1                             2                      True   \n",
       "5                   2                             2                      True   \n",
       "6                   3                             2                      True   \n",
       "7                   4                             2                      True   \n",
       "8                   1                             3                      True   \n",
       "9                   2                             3                      True   \n",
       "10                  3                             3                      True   \n",
       "11                  4                             3                      True   \n",
       "\n",
       "          F1  Precision    Recall  \n",
       "0   0.435208   0.967391  0.280757  \n",
       "1   0.542793   0.948819  0.380126  \n",
       "2   0.576419   0.936170  0.416404  \n",
       "3   0.575107   0.899329  0.422713  \n",
       "4   0.435741   0.972678  0.280757  \n",
       "5   0.561404   0.920863  0.403785  \n",
       "6   0.594480   0.909091  0.441640  \n",
       "7   0.589823   0.863222  0.447950  \n",
       "8   0.456221   0.846154  0.312303  \n",
       "9   0.562105   0.844937  0.421136  \n",
       "10  0.596741   0.841954  0.462145  \n",
       "11  0.601610   0.830556  0.471609  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_edit_distance = [1, 2, 3]\n",
    "weight_of_bad_word = [True]\n",
    "bad_words_lists = [bad_words_original, bad_words_bgjargon_1, bad_words_bgjargon_2, bad_words_bgjargon_3]\n",
    "results = {\n",
    "    \"Bad words set no.\": [],\n",
    "    \"Max dictionary edit distance\": [],\n",
    "    \"More weight of bad words\": [],\n",
    "    \"F1\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": []\n",
    "}\n",
    "\n",
    "for w in weight_of_bad_word:\n",
    "    for e in max_edit_distance:\n",
    "        for i, b in enumerate(bad_words_lists):\n",
    "            precision, recall = score_of_word_similarity_annotation_with_symspell_suggestions(e, w, b)\n",
    "            f1 = 2*precision*recall/(precision + recall)\n",
    "\n",
    "            print(f\"bad words set={i+1} max edit distance={e} weight={w} -> {f1, precision, recall}\")\n",
    "            results[\"Bad words set no.\"].append(i+1)\n",
    "            results[\"Max dictionary edit distance\"].append(e)\n",
    "            results[\"More weight of bad words\"].append(w)\n",
    "            results[\"F1\"].append(f1)\n",
    "            results[\"Precision\"].append(precision)\n",
    "            results[\"Recall\"].append(recall)\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
